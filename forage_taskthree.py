# -*- coding: utf-8 -*-
"""Forage-TaskThree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o7vZmXeoveCErECnF495z1VOa-ibjLBb
"""

#Data preprocessing

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

loan_data = pd.read_csv('Task 3 and 4_Loan_Data.csv')
loan_data = loan_data.drop('customer_id', axis=1)
loan_data.head()

loan_data.isna().sum() #no missing values

loan_data['debt-to-income'] = loan_data['total_debt_outstanding']/loan_data['income']

loan_data.corr()

from pandas.plotting import scatter_matrix
scatter_matrix(loan_data, figsize=(15,15))
plt.show()

"""From the scatterplot matrix, it can been seen that there is a linear relationship between income and the oustanding amount of loan, between the debt-to-income ratio and number of outstanding credit lines, between number of outstanding credit lines and outstanding debt amount. There are also some variables having a high correlation with the decision to default on the loan or not, including number of oustanding credit lines, debt-to-income ratio, oustanding debt amount and FICO score. In addition, the total_debt_oustanding variable is right skewed, leading to the debt-to-income ratio being right skewed as well."""

#Detect and remove outlier
plt.figure()
sns.boxplot(loan_data['loan_amt_outstanding'])
plt.figure()
sns.boxplot(loan_data['total_debt_outstanding'])
plt.figure()
sns.boxplot(loan_data['income'])
plt.figure()
sns.boxplot(loan_data['debt-to-income'])

Upper = loan_data.quantile(0.75) + 1.5*(loan_data.quantile(0.75)-loan_data.quantile(0.25))
Lower = loan_data.quantile(0.25) - 1.5*(loan_data.quantile(0.75)-loan_data.quantile(0.25))

Upper = Upper[['loan_amt_outstanding', 'debt-to-income']]
Lower = Lower[['loan_amt_outstanding', 'debt-to-income']]

loan_data = loan_data.loc[(loan_data['loan_amt_outstanding'] > Lower[0]) & (loan_data['loan_amt_outstanding'] < Upper[0]) & (loan_data['debt-to-income'] > Lower[1]) ]
loan_data.reset_index(drop=True, inplace=True)
#Scale and normalize
from sklearn.preprocessing import MinMaxScaler
scaled_data = loan_data[['credit_lines_outstanding', 'loan_amt_outstanding', 'years_employed', 'fico_score', 'debt-to-income']]
scaler = MinMaxScaler()
scaled_data = pd.DataFrame(scaler.fit_transform(scaled_data),columns=scaled_data.columns)

import numpy as np
scaled_data['debt-to-income'] = np.sqrt(scaled_data['debt-to-income'])

scaled_data = pd.concat([scaled_data, loan_data['default']], axis=1, ignore_index=True)
scaled_data.columns = ['credit_lines_outstanding', 'loan_amt_outstanding', 'years_employed', 'fico_score', 'debt-to-income', 'default']

from sklearn.decomposition import PCA
pca = PCA(n_components=4)
X_pca = pca.fit_transform(scaled_data.drop('default', axis=1))

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X_pca, scaled_data.default, test_size = 0.3,
    random_state=42, stratify=scaled_data.default)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

log = LogisticRegression()
log.fit(X_train, y_train)
y_pred = log.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

from xgboost import XGBClassifier
xgboost = XGBClassifier()
xgboost.fit(X_train, y_train)
y_pred_2 = xgboost.predict(X_test)

print(accuracy_score(y_test, y_pred_2))
print(confusion_matrix(y_test, y_pred_2))
print(classification_report(y_test, y_pred_2))

"""From the results, both models provide good predictions for the data. Since the XGBoost accuracy score is slightly higher, we will use this model to predict the probability of default for new data."""

loan_data['PD'] = xgboost.predict_proba(X_pca)[:,1]
loan_data.head(10)

def predict(credit_lines_outstanding, loan_amt_outstanding, total_debt_outstanding, income, years_employed, fico_score):
    recovery_rate = 0.1
    debt_to_income = total_debt_outstanding/income
    input_data = pd.DataFrame([[credit_lines_outstanding, loan_amt_outstanding, years_employed, fico_score, debt_to_income]])
    scaled_input = pd.DataFrame(scaler.transform(input_data),columns=scaled_data.columns[0:5])
    scaled_input['debt-to-income'] = np.sqrt(scaled_input['debt-to-income'])
    scaled_input = pca.transform(scaled_input)
    prob_default = xgboost.predict_proba(scaled_input)[:,1][0]
    expected_loss = prob_default*(1-recovery_rate)*loan_amt_outstanding
    return expected_loss
predict(5, 1958, 8228, 26650, 2, 572)